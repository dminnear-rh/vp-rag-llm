replicaCount: 1

image:
  repository: vllm/vllm-openai
  tag: latest
  pullPolicy: IfNotPresent

model:
  name: mistralai/Mistral-7B-Instruct-v0.3
  servedName: mistral
  dtype: bfloat16
  tokenizerMode: mistral

extraArgs: []  # e.g., ["--max-model-len=32768", "--disable-log-requests"]

resources:
  limits:
    nvidia.com/gpu: 1
    cpu: "4"
    memory: 32Gi
  requests:
    nvidia.com/gpu: 1
    cpu: "2"
    memory: 24Gi

persistence:
  enabled: true
  size: 30Gi
  accessModes: ["ReadWriteOnce"]
  mountPath: /.cache

nodeSelector:
  nvidia.com/gpu.present: "true"

service:
  type: ClusterIP
  port: 8000

secretStore:
  name: vault-backend
  kind: ClusterSecretStore
  remoteKey: secret/data/global/hfmodel

# model:
#   name: meta-llama/Llama-4-Scout-17B-16E-Instruct
#   servedName: llama-4
#   dtype: bfloat16
#   tokenizerMode: auto

# resources:
#   limits:
#     nvidia.com/gpu: 4
#     cpu: "6"
#     memory: 64Gi
#   requests:
#     nvidia.com/gpu: 4
#     cpu: "4"
#     memory: 48Gi
